{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from numerapi import NumerAPI\n",
    "import traceback\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "napi = NumerAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/MTL_Assignment1_MT23028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd MTL_Assignment1_MT23028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'Assignment1_config.yaml'\n",
    "\n",
    "# Read Configuration file\n",
    "with open(config_file, \"r\") as F: configs = yaml.safe_load(F); F.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:53:45,648 INFO numerapi.utils: starting download\n",
      "data/live.parquet: 7.83MB [00:00, 12.0MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "feature_set = json.load(open(f\"data/{configs['Dataset']['name']}/features.json\"))[\"feature_sets\"][f\"{configs['Dataset']['set']}\"]\n",
    "\n",
    "napi.download_dataset(filename = f\"{configs['Dataset']['name']}/live.parquet\", dest_path = f\"data/live.parquet\")\n",
    "\n",
    "live_dataset = pd.read_parquet(f\"data/{configs['Dataset']['name']}/live.parquet\",columns = feature_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_module(module, updates=None, memo=None):\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "    if updates is not None:\n",
    "        params = list(module.parameters())\n",
    "        if not len(updates) == len(list(params)):\n",
    "            msg = 'WARNING:update_module(): Parameters and updates have different length. ('\n",
    "            msg += str(len(params)) + ' vs ' + str(len(updates)) + ')'\n",
    "            print(msg)\n",
    "        for p, g in zip(params, updates):\n",
    "            p.update = g\n",
    "\n",
    "    # Update the params\n",
    "    for param_key in module._parameters:\n",
    "        p = module._parameters[param_key]\n",
    "        if p in memo:\n",
    "            module._parameters[param_key] = memo[p]\n",
    "        else:\n",
    "            if p is not None and hasattr(p, 'update') and p.update is not None:\n",
    "                updated = p + p.update\n",
    "                p.update = None\n",
    "                memo[p] = updated\n",
    "                module._parameters[param_key] = updated\n",
    "\n",
    "    # Second, handle the buffers if necessary\n",
    "    for buffer_key in module._buffers:\n",
    "        buff = module._buffers[buffer_key]\n",
    "        if buff in memo:\n",
    "            module._buffers[buffer_key] = memo[buff]\n",
    "        else:\n",
    "            if buff is not None and hasattr(buff, 'update') and buff.update is not None:\n",
    "                updated = buff + buff.update\n",
    "                buff.update = None\n",
    "                memo[buff] = updated\n",
    "                module._buffers[buffer_key] = updated\n",
    "\n",
    "    # Then, recurse for each submodule\n",
    "    for module_key in module._modules:\n",
    "        module._modules[module_key] = update_module(\n",
    "            module._modules[module_key],\n",
    "            updates=None,\n",
    "            memo=memo,\n",
    "        )\n",
    "\n",
    "    if hasattr(module, 'flatten_parameters'):\n",
    "        module._apply(lambda x: x)\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_module(module, memo=None):\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "\n",
    "    if not isinstance(module, torch.nn.Module):\n",
    "        return module\n",
    "    clone = module.__new__(type(module))\n",
    "    clone.__dict__ = module.__dict__.copy()\n",
    "    clone._parameters = clone._parameters.copy()\n",
    "    clone._buffers = clone._buffers.copy()\n",
    "    clone._modules = clone._modules.copy()\n",
    "\n",
    "    # Second, re-write all parameters\n",
    "    if hasattr(clone, '_parameters'):\n",
    "        for param_key in module._parameters:\n",
    "            if module._parameters[param_key] is not None:\n",
    "                param = module._parameters[param_key]\n",
    "                param_ptr = param.data_ptr\n",
    "                if param_ptr in memo:\n",
    "                    clone._parameters[param_key] = memo[param_ptr]\n",
    "                else:\n",
    "                    cloned = param.clone()\n",
    "                    clone._parameters[param_key] = cloned\n",
    "                    memo[param_ptr] = cloned\n",
    "\n",
    "    # Third, handle the buffers if necessary\n",
    "    if hasattr(clone, '_buffers'):\n",
    "        for buffer_key in module._buffers:\n",
    "            if clone._buffers[buffer_key] is not None and \\\n",
    "                    clone._buffers[buffer_key].requires_grad:\n",
    "                buff = module._buffers[buffer_key]\n",
    "                buff_ptr = buff.data_ptr\n",
    "                if buff_ptr in memo:\n",
    "                    clone._buffers[buffer_key] = memo[buff_ptr]\n",
    "                else:\n",
    "                    cloned = buff.clone()\n",
    "                    clone._buffers[buffer_key] = cloned\n",
    "                    memo[buff_ptr] = cloned\n",
    "\n",
    "    # Then, recurse for each submodule\n",
    "    if hasattr(clone, '_modules'):\n",
    "        for module_key in clone._modules:\n",
    "            clone._modules[module_key] = clone_module(\n",
    "                module._modules[module_key],\n",
    "                memo=memo,\n",
    "            )\n",
    "\n",
    "    if hasattr(clone, 'flatten_parameters'):\n",
    "        clone = clone._apply(lambda x: x)\n",
    "    return clone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLearner(nn.Module):\n",
    "    def __init__(self, module=None):\n",
    "        super(BaseLearner, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        try:\n",
    "            return super(BaseLearner, self).__getattr__(attr)\n",
    "        except AttributeError:\n",
    "            return getattr(self.__dict__['_modules']['module'], attr)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.module(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml_update(model, lr, grads=None):\n",
    "    if grads is not None:\n",
    "        params = list(model.parameters())\n",
    "        if not len(grads) == len(list(params)):\n",
    "            msg = 'WARNING:maml_update(): Parameters and gradients have different length. ('\n",
    "            msg += str(len(params)) + ' vs ' + str(len(grads)) + ')'\n",
    "            print(msg)\n",
    "        for p, g in zip(params, grads):\n",
    "            if g is not None:\n",
    "                p.update = - lr * g\n",
    "    return update_module(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML(BaseLearner):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 lr,\n",
    "                 first_order=False,\n",
    "                 allow_unused=None,\n",
    "                 allow_nograd=False):\n",
    "        super(MAML, self).__init__()\n",
    "        self.module = model\n",
    "        self.lr = lr\n",
    "        self.first_order = first_order\n",
    "        self.allow_nograd = allow_nograd\n",
    "        if allow_unused is None:\n",
    "            allow_unused = allow_nograd\n",
    "        self.allow_unused = allow_unused\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.module(*args, **kwargs)\n",
    "\n",
    "    def adapt(self,\n",
    "              loss,\n",
    "              first_order=None,\n",
    "              allow_unused=None,\n",
    "              allow_nograd=None):\n",
    "\n",
    "        if first_order is None:\n",
    "            first_order = self.first_order\n",
    "        if allow_unused is None:\n",
    "            allow_unused = self.allow_unused\n",
    "        if allow_nograd is None:\n",
    "            allow_nograd = self.allow_nograd\n",
    "        second_order = not first_order\n",
    "\n",
    "        if allow_nograd:\n",
    "            # Compute relevant gradients\n",
    "            diff_params = [p for p in self.module.parameters() if p.requires_grad]\n",
    "            grad_params = grad(loss,\n",
    "                               diff_params,\n",
    "                               retain_graph=second_order,\n",
    "                               create_graph=second_order,\n",
    "                               allow_unused=allow_unused)\n",
    "            gradients = []\n",
    "            grad_counter = 0\n",
    "\n",
    "            # Handles gradients for non-differentiable parameters\n",
    "            for param in self.module.parameters():\n",
    "                if param.requires_grad:\n",
    "                    gradient = grad_params[grad_counter]\n",
    "                    grad_counter += 1\n",
    "                else:\n",
    "                    gradient = None\n",
    "                gradients.append(gradient)\n",
    "        else:\n",
    "            try:\n",
    "                gradients = grad(loss,\n",
    "                                 self.module.parameters(),\n",
    "                                 retain_graph=second_order,\n",
    "                                 create_graph=second_order,\n",
    "                                 allow_unused=allow_unused)\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print('learn2learn: Maybe try with allow_nograd=True and/or allow_unused=True ?')\n",
    "\n",
    "        # Update the module\n",
    "        self.module = maml_update(self.module, self.lr, gradients)\n",
    "\n",
    "    def clone(self, first_order=None, allow_unused=None, allow_nograd=None):\n",
    "        if first_order is None:\n",
    "            first_order = self.first_order\n",
    "        if allow_unused is None:\n",
    "            allow_unused = self.allow_unused\n",
    "        if allow_nograd is None:\n",
    "            allow_nograd = self.allow_nograd\n",
    "        return MAML(clone_module(self.module),\n",
    "                    lr=self.lr,\n",
    "                    first_order=first_order,\n",
    "                    allow_unused=allow_unused,\n",
    "                    allow_nograd=allow_nograd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=2, dropout=0.1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Linear layer to project input features to the LSTM model dimension (hidden_dim)\n",
    "        self.feature_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input features to LSTM model dimension\n",
    "        x = self.feature_embedding(x)\n",
    "        x = x.unsqueeze(1)  # [batch_size, input_dim] -> [batch_size, 1, hidden_dim]\n",
    "        x, (h_n, c_n) = self.lstm(x)  # x has shape [batch_size, seq_len, hidden_dim]\n",
    "        x = x[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        x = torch.sigmoid(self.fc_out(x))  # Output between 0 and 1 for binary classification\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(42,1)\n",
    "\n",
    "maml = MAML(model, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved model\n",
    "maml.load_state_dict(torch.load(f\"saved_models/{configs['Experiment_Name']}/{configs['Model']['name']}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_predictions = maml(torch.tensor(live_dataset[feature_set].values, dtype=torch.float32)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('predictions', exist_ok=True)\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "filename = f\"{date_str}_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.Series(live_predictions.detach().numpy(), index=live_dataset.index).to_frame(f'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'predictions/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
