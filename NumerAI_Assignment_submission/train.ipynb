{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd MTL_Assignment1_MT23028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "config_file = 'Assignment1_config.yaml'\n",
    "# Read Configuration file\n",
    "with open(config_file, \"r\") as F: \n",
    "    configs = yaml.safe_load(F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_set = json.load(open(f\"data/{configs['Dataset']['name']}/features.json\"))[\"feature_sets\"][f\"{configs['Dataset']['set']}\"]\n",
    "\n",
    "# Load Train Dataset\n",
    "train_dataset = pd.read_parquet(f\"data/{configs['Dataset']['name']}/train.parquet\",columns = [\"era\", \"target\"] + feature_set)\n",
    "# Reduce Dataset size\n",
    "# train = pd.DataFrame(train_dataset[train_dataset[\"era\"].isin(pd.Series(train_dataset[\"era\"].unique()[::configs['Dataset'][configs['Dataset']['name']]['reduce_dataset_size']]))])\n",
    "# train = pd.DataFrame(train_dataset[train_dataset[\"era\"].isin(pd.Series(train_dataset[\"era\"].unique()[-300:]))])\n",
    "train = pd.DataFrame(train_dataset[train_dataset[\"era\"].isin(pd.Series(train_dataset[\"era\"]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save last train era information in config file\n",
    "configs[\"Train\"][\"last_train_era\"] = int(train[\"era\"].unique()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=2, dropout=0.1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear layer to project input features to the LSTM model dimension (hidden_dim)\n",
    "        self.feature_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project input features to LSTM model dimension\n",
    "        x = self.feature_embedding(x)\n",
    "        # Reshape input for LSTM (batch_first=True, so shape should be [batch_size, seq_len, hidden_dim])\n",
    "        x = x.unsqueeze(1)  # [batch_size, input_dim] -> [batch_size, 1, hidden_dim]\n",
    "        # LSTM forward pass\n",
    "        x, (h_n, c_n) = self.lstm(x)  # x has shape [batch_size, seq_len, hidden_dim]\n",
    "        # Get the output for the last time step (if sequence length = 1, we use the only time step)\n",
    "        x = x[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        # Output layer to match target dimension\n",
    "        x = torch.sigmoid(self.fc_out(x))  # Output between 0 and 1 for binary classification\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LSTMModel(input_dim=42, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maml = train_maml(net = model, data_loader_list=prepare_data_loader_list(configs),epochs=100,fast_adaptation_steps=5,inner_lr=1e-2,outer_lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exist\n",
    "if not os.path.isdir(f\"saved_models/{configs['Experiment_Name']}\"):\n",
    "    os.makedirs(f\"saved_models/{configs['Experiment_Name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(maml.state_dict(),f\"saved_models/{configs['Experiment_Name']}/{configs['Model']['name']}.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
