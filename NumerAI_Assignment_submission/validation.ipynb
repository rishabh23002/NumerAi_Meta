{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import learn2learn as l2l\n",
    "from helper import validate_maml\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/MTL_Assignment1_MT23028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd MTL_Assignment1_MT23028/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Configuration file\n",
    "with open('Assignment1_config.yaml', \"r\") as F: configs = yaml.safe_load(F); F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = json.load(open(f\"data/{configs['Dataset']['name']}/features.json\"))[\"feature_sets\"][f\"{configs['Dataset']['set']}\"]\n",
    "\n",
    "# Load Validation dataset\n",
    "validation_dataset = pd.read_parquet(f\"data/{configs['Dataset']['name']}/validation.parquet\",columns = [\"era\", \"data_type\", \"target\"] + feature_set)\n",
    "\n",
    "validation_dataset = pd.DataFrame(validation_dataset[validation_dataset[\"data_type\"] == \"validation\"])\n",
    "del validation_dataset[\"data_type\"]\n",
    "\n",
    "# validation_dataset = pd.DataFrame(validation_dataset[validation_dataset[\"era\"].isin(pd.Series(validation_dataset[\"era\"].unique()[::configs['Dataset'][configs['Dataset']['name']]['reduce_dataset_size']]))])\n",
    "validation_dataset = pd.DataFrame(validation_dataset[validation_dataset[\"era\"].isin(pd.Series(validation_dataset[\"era\"]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "if configs[\"Train\"][\"last_train_era\"] == -1:\n",
    "    # Load Train Dataset\n",
    "    train_dataset = pd.read_parquet(f\"data/{configs['Dataset']['name']}/train.parquet\",columns = [\"era\", \"target\"] + feature_set)\n",
    "    # Reduce Dataset size\n",
    "    train = pd.DataFrame(train_dataset[train_dataset[\"era\"].isin(pd.Series(train_dataset[\"era\"]))])\n",
    "    configs[\"Train\"][\"last_train_era\"] = int(train[\"era\"].unique()[-1])\n",
    "\n",
    "last_train_era = configs[\"Train\"][\"last_train_era\"]\n",
    "\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i+1 for i in range(4)]]\n",
    "validation = pd.DataFrame(validation_dataset[~validation_dataset[\"era\"].isin(eras_to_embargo)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader_list = []\n",
    "for era in sorted(validation['era'].unique()):\n",
    "    era_df = pd.DataFrame(validation[validation['era'] == era])\n",
    "\n",
    "    X = era_df[feature_set].values  # Features\n",
    "    y = era_df['target'].values  # Target\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Create dataset and loader for the entire validation set of this task\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    test_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=configs['Validation']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Append only the test loader as there's no separate train loader for validation\n",
    "    validation_loader_list.append(test_loader)  # Using None for train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=2, dropout=0.1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Linear layer to project input features to the LSTM model dimension (hidden_dim)\n",
    "        self.feature_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_embedding(x)\n",
    "        x = x.unsqueeze(1)  # [batch_size, input_dim] -> [batch_size, 1, hidden_dim]\n",
    "        x, (h_n, c_n) = self.lstm(x)  # x has shape [batch_size, seq_len, hidden_dim]\n",
    "        x = x[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        x = torch.sigmoid(self.fc_out(x))  # Output between 0 and 1 for binary classification\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=42, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml = l2l.algorithms.MAML(model, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "maml.load_state_dict(torch.load(f\"saved_models/{configs['Experiment_Name']}/{configs['Model']['name']}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_maml(maml=maml, validation_loader_list=validation_loader_list, validation=validation,configs=configs, n_inner_steps=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
